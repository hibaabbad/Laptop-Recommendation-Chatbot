{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Manufacturer: Apple\n",
      "Model Name: MacBook Pro\n",
      "Category: Ultrabook\n",
      "Screen Size: 13.3\"\n",
      "Screen: IPS Panel Retina Display 2560x1600\n",
      "CPU: Intel Core i5 2.3GHz\n",
      "RAM: 8GB\n",
      "Storage: 128GB SSD\n",
      "GPU: Intel Iris Plus Graphics 640\n",
      "Operating System: macOS\n",
      "Operating System Version: \n",
      "Weight: 1.37kg\n",
      "Price (Euros): 1339,69' metadata={'source': 'laptops.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "#1 load the data\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "#A document loader\n",
    "loader = CSVLoader('laptops.csv', encoding='ISO-8859-1')\n",
    "\n",
    "#Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Manufacturer: Apple\n",
      "Model Name: MacBook Pro\n",
      "Category: Ultrabook\n",
      "Screen Size: 13.3\"\n",
      "Screen: IPS Panel Retina Display 2560x1600\n",
      "CPU: Intel Core i5 3.1GHz\n",
      "RAM: 8GB\n",
      "Storage: 256GB SSD\n",
      "GPU: Intel Iris Plus Graphics 650\n",
      "Operating System: macOS\n",
      "Operating System Version: \n",
      "Weight: 1.37kg\n",
      "Price (Euros): 1803,60' metadata={'source': 'laptops.csv', 'row': 4}\n"
     ]
    }
   ],
   "source": [
    "#split the data into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(separators=['\\n\\n', '\\n', ': '],chunk_size=500,chunk_overlap=50)\n",
    "docs = splitter.split_documents(data)\n",
    "\n",
    "print(docs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12820\\2575904129.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\HP\\Documents\\Stage - Cetic\\Code\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\HP\\Documents\\Stage - Cetic\\Code\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#set the embedding\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Chroma vector store and retriever\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the llm\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not set\")\n",
    "\n",
    "#load the model \n",
    "llm_groq = ChatGroq(\n",
    "    groq_api_key=api_key,\n",
    "    model_name='llama3-8b-8192'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history aware retriever setup\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history where a user is looking for a laptop based on specific requirements, \"\n",
    "    \"formulate a standalone question that clearly identifies the user's laptop needs without referring \"\n",
    "    \"to previous chat messages. The goal is to ensure the question can be understood and processed \"\n",
    "    \"independently. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm_groq, retriever, contextualize_q_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question-answer chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant specialized in helping users find the best laptop from the available data based on their requirements. \"\n",
    "    \"Use the provided laptop specifications and user preferences to recommend the most suitable options. \"\n",
    "    \"do not use sentences like this Based on the provided data instead you can say in our store thats what we have\"\n",
    "    \"If the user's requirements cannot be fully met, suggest the closest alternatives available. \"\n",
    "    \"Keep your answers concise and focused on the laptop's key features such as price, performance, battery life, and portability. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm_groq, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In our store, I\\'d recommend the Dell Inspiron 5379 2 in 1 Convertible for design purposes. Its 13.3\" Full HD touchscreen display and Intel Core i5 processor provide a great balance of portability and performance for creative tasks. The 8GB RAM and 1TB HDD storage ensure smooth multitasking and ample storage for your files.\\n\\nHowever, keep in mind that the Intel UHD Graphics 620 might not be as powerful as a dedicated graphics card, which might be a limitation for more demanding design tasks. But for general graphic design, video editing, and other creative work, this laptop should suffice.\\n\\nIf you\\'d like a larger screen, the Dell Inspiron 5579 2 in 1 Convertible is also a great option, but it\\'s slightly heavier and more expensive.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is The best laptop for design ?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  \n",
    ")[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
